\section{Conclusions}

In this paper, we have analyzed the performance of two classifiers, \emph{Nearest Neighbor} and \emph{Support Vector Machine}, with different distance functions and kernel functions, using the \emph{Bag of Words} model. 

The experiments done on the dataset lead to the conclusion that there are many parameters that can influence the performance of a classifier.
The greater the dimension of the training set, the better the classification accuracy is obtained, because more examples lead to a more accurate abstraction of the data model; more codewords improves the performance due to the fact that a larger vocabulary means a more expressive one and so we have a histogram more capable of discriminating between images (however, the accuracy boost of a larger vocabulary eventually saturates\cite{Philbin08lostin}). The choice of sparse or dense local features, on which the SIFT descriptors are computed, can also influence the classification accuracy, as shown in Table \ref{tab:sifttype}.

Finally, we have analyzed how different assignment in feature quantization can influence the performance of a classifier, in particular the difference between \emph{hard-assignment} and \emph{soft-assignment}.
We have found that by allowing a degree of ambiguity in feature quantization, \emph{soft-assignment} is able to improve the classification accuracy of \emph{hard-assignment}. Improvement over the \emph{soft-assignment} can be obtain using the \emph{localized soft-assignment} by taking into account only the k-nearest codewords in order to remove some noise in the features quantization.