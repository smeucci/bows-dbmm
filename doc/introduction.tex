\section{Introduction}

In computer vision, the \emph{bag-of-words} model is one of the well established image classification methods. Not only comparing two image by matching each local features of the images is computationally intensive, but also different images have different number of local features. BoW model simplified this process by building a vocabulary of visual-words, called codebook. This way, an image descriptor is a histogram built by counting how many codewords are present in an image.

Given this model for image classification, we have compared how different classifiers performs.
We have studied the \emph{Nearest-Neighbor} (NN) classifier, comparing the results using both the Euclidean distance and the $\chi^2$ distance.

In addiction, we have examined the \emph{Support Vector Machine} (SVM) classifier, implementing different kernel functions; in particular, the linear kernel, the intersection kernel, the $\chi^2$ kernel, and the radial basis function kernel have been implemented.
In evaluating the performance of a classifiers, many parameters, that can vary the results, must be taken into account.
For instance, we have also compared the results on varying parameters like the density or sparseness of the key-points in each image, the dimension of the vocabulary and the dimension of the training set used to train the classifiers.

Another aspect that can influence the performance of a classifier, using the BoW model, is choosing between \emph{hard-assignment} and \emph{soft-assignment}. Briefly, with \emph{hard-assignment} each local features of an image is assigned to one codeword; with \emph{soft-assignment} each local features is assigned to every codeword of the vocabulary with some weights that give a measure of how strong is each assignment. We give more details in Section 3.

An image descriptor is computed by following this pipeline:
\begin{enumerate}
\item Given an image, dense or sparse local features are extracted and described using SIFT descriptor.
\item Using every SIFT descriptor of all images, a vocabulary of visual words is built by the k-means algorithm; the centroids of each clusters will be the codewords of the vocabulary.
\item Given an image, for each SIFT descriptor, feature quantization is performed using hard-assignment or soft-assignment, based on the computed codebook.
\item Finally, for each image, a histogram is built, with each bin stating how much of the corresponding codeword is present in an image.
\end{enumerate}

Once the histograms are computed for each images of a given dataset, the image descriptors are ready for classification.
